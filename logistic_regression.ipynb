{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkestelman/jax-ml-tutorial/blob/master/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHbUfcDMn2ok",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression with JAX Tutorial**\n",
        "\n",
        "We're going to build a simple logistic regression from scratch while letting JAX take care of calculating derivatives during the learning process.\n",
        "\n",
        "The main function we want from JAX is jax.grad(), which will automatically calculate gradients for us.\n",
        "\n",
        "We also import jax.numpy, which wraps numpy functions so that jax can calculate their derivatives while letting us use good ol' familiar numpy. \n",
        "\n",
        "We import regular numpy as onp (original numpy). We don't really need this, but it is convenient in a few special cases (like random numbers). We discuss this more later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6YRmSlKSwVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax import grad\n",
        "import numpy as onp # original numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7JgUSR9p4dJ",
        "colab_type": "text"
      },
      "source": [
        "Simple logistic regression (observe how we use jax.grad for back-propagation!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XdYqKcCtzlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deW9n0hYS8q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, W, b):\n",
        "  return sigmoid(W.dot(x) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy5f9rtGTMJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(x, target, W, b):\n",
        "  \"\"\"Negative log loss\"\"\"\n",
        "  pred = forward(x, W, b)\n",
        "  pred = pred[0]\n",
        "  return -np.sum(target * np.log(pred) + (1-target) * np.log(1-pred)) # np.sum here is just to convert from 0d array to scalar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSSCzPoyTm7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(x, target, W, b, learning_rate):\n",
        "  W -= grad(loss, argnums=2)(x, target, W, b) * learning_rate\n",
        "  b -= grad(loss, argnums=3)(x, target, W, b) * learning_rate\n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjSrTJ4qMgj",
        "colab_type": "text"
      },
      "source": [
        "Initialize parameters (weight matrix and bias)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxRM_zMrUjMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_params(input_size, output_size):\n",
        "  W = onp.random.rand(output_size, input_size)\n",
        "  b = 0.0\n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRhuur_7XcEN",
        "colab_type": "code",
        "outputId": "e1f573e5-7f6a-4ce9-ea2f-3c7109e1cfa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W, b = init_params(2, 1)\n",
        "print(W, b)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.52840709 0.83502896]] 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-oEbiItqSPn",
        "colab_type": "text"
      },
      "source": [
        "Define how to train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0yKMbWhVJ4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X, labels, W, b, learning_rate):\n",
        "  for x, label in zip(X, labels):\n",
        "    W, b = backprop(x, label, W, b, learning_rate)\n",
        "  return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVYIO9KSZ3km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(X, labels, W, b):\n",
        "  correct = 0\n",
        "  for x, label in zip(X, labels):\n",
        "    raw_pred = forward(x, W, b)\n",
        "    pred = 0 if raw_pred < 0.5 else 1\n",
        "    if pred == label:\n",
        "      correct += 1\n",
        "  print('Accuracy:', correct / len(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exob-qokqhtt",
        "colab_type": "text"
      },
      "source": [
        "We will test if our logistic regression model can learn to solve boolean AND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyDiSgReXgXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 200\n",
        "X = [ onp.random.randint(2, size=2) for _ in range(N) ]\n",
        "labels = [ np.array([float(X[0] and X[1])]) for X in X ]\n",
        "\n",
        "train_test_split = int(0.7 * N)\n",
        "train_X, train_labels = X[:train_test_split], labels[:train_test_split]\n",
        "test_X, test_labels = X[train_test_split:], labels[train_test_split:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRlRhbbzquSG",
        "colab_type": "text"
      },
      "source": [
        "First we test how it does without any training. This result will vary depending on the initial parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaMFexmpgice",
        "colab_type": "code",
        "outputId": "6607de79-d72a-4cd8-d995-ed609608ec2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test(test_X, test_labels, W, b)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ypJjRY3rCCR",
        "colab_type": "text"
      },
      "source": [
        "Train the model and show the weights and bias after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KvDhQ_jZGYE",
        "colab_type": "code",
        "outputId": "50087650-45b8-417b-8768-70478393f10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W, b = train(train_X, train_labels, W, b, learning_rate=0.1)\n",
        "print(W, b)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.86102784 1.1675522 ]] -1.9526529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eqbfRIgrGTJ",
        "colab_type": "text"
      },
      "source": [
        "Test again after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bwp5JOZdZYE",
        "colab_type": "code",
        "outputId": "68a7e683-3109-4aa7-bc56-35ae180e3a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test(test_X, test_labels, W, b)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECRQX_duwBEj",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "We learned how to use jax.grad() to automatically calculate derivates for us and saw it in action during back-propagation in a logistic regression model. \n",
        "\n",
        "An important note that we didn't mention is that jax.grad only works with pure functions (functions that do not change or depend on external state such as global variables). This means you cannot directly put these functions in a LogisticRegression class and use `self` to store and access parameters instead of passing them as arguments to each function (this would violate purity). \n",
        "\n",
        "We will discuss ways to work around this limitation in another tutorial, but we encourage you to try the more functional approach instead. One suggestion is to pass around a params dict instead of (W, b) everywhere as we have here. \n",
        "\n",
        "Note this is the reason jax.numpy doesn't have the same random functionality as numpy - since it requires accessing a random state so it is inherently impure. jax.numpy does have random functions, but they always require a seed - this way they evaluate the same regardless of context (making them pure). (source: https://sjmielke.com/jax-purify.htm)"
      ]
    }
  ]
}